{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lightwsrld/pytorch/blob/main/transformer_model_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZzSFU1p4bKtG",
        "outputId": "729f4671-e894-46ec-8099-c9a43dff587e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchtext==0.6.0\n",
            "  Downloading torchtext-0.6.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m989.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.27.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.22.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from torchtext==0.6.0) (1.16.0)\n",
            "Collecting sentencepiece (from torchtext==0.6.0)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.6.0) (3.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchtext==0.6.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchtext==0.6.0) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchtext==0.6.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchtext==0.6.0) (1.3.0)\n",
            "Installing collected packages: sentencepiece, torchtext\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.15.2\n",
            "    Uninstalling torchtext-0.15.2:\n",
            "      Successfully uninstalled torchtext-0.15.2\n",
            "Successfully installed sentencepiece-0.1.99 torchtext-0.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install torchtext==0.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nD8HNu7nbMNY",
        "outputId": "2f8c1abc-dc28-4371-c71a-7e45a1c5315a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available device: cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os,gc,re,time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import spacy,random\n",
        "from collections import Counter\n",
        "import warnings,unicodedata\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "\n",
        "#setting device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Available device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifot9laabe8s",
        "outputId": "d13193cf-bfe4-4c16-d2e2-af422b897d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mu4vsBj5bpDg"
      },
      "outputs": [],
      "source": [
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6OfQu4Abrph",
        "outputId": "b7955d1e-20d4-41b2-cb15-de6d2cc31df3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29000, 2) (1014, 2) (1000, 2)\n"
          ]
        }
      ],
      "source": [
        "train_csv_path, val_csv_path, test_csv_path = 'train_1.csv','val_1.csv','test_1.csv'\n",
        "train_data = pd.read_csv(train_csv_path)\n",
        "val_data = pd.read_csv(val_csv_path)\n",
        "test_data = pd.read_csv(test_csv_path)\n",
        "\n",
        "print(train_data.shape, val_data.shape, test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUQ0smv3b5Jw"
      },
      "outputs": [],
      "source": [
        "import re,unicodedata\n",
        "import time,datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfWYiXa_b1pD",
        "outputId": "eff3d049-e17d-4c73-af36-a19ae3b91f79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-08 02:27:39.397869: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting de-core-news-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.3)\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-3.5.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('de_core_news_sm')\n",
            "2023-08-08 02:27:54.680565: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Collecting en-core-web-sm==3.5.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.5.0) (3.5.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.65.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.27.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.12)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.1.0)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download de_core_news_sm\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DKqCA5nclSQ"
      },
      "outputs": [],
      "source": [
        "def unicodeToAscii(s):\n",
        "\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "replace_dict = {\"don't\": \"do not\",\"shouldn't\": \"should not\",\n",
        "         \"can't\": \"cannot\", \"I'm\": \"Iam\", \"I've\": \"I have\",\n",
        "         \"haven't\": \"have not\",\n",
        "         \"hasn't\": \"has not\",\n",
        "         \"didn't\": \"did not\"}\n",
        "\n",
        "def clean_eng_text(text, replace_dict=replace_dict):\n",
        "\n",
        "    text = unicodeToAscii(text.lower().strip())\n",
        "    text = re.sub('[-?<>.!]', '', text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = ' '.join([replace_dict[word] if word in replace_dict.keys() else word for word in text.split()])\n",
        "    text = [re.sub('[^A-Za-z]', '', word) for word in text.split()]\n",
        "    return ' '.join(text).lower()\n",
        "\n",
        "def clean_ger_text(text):\n",
        "\n",
        "    text = text.strip()\n",
        "    text = re.sub(\"[.?]\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5XPm98Mcomo"
      },
      "outputs": [],
      "source": [
        "train_data[\"English\"] = train_data[\"English\"].apply(lambda x: clean_eng_text(x))\n",
        "train_data[\"Ger\"] = train_data[\"Ger\"].apply(lambda x: clean_ger_text(x))\n",
        "\n",
        "val_data[\"English\"] = val_data[\"English\"].apply(lambda x: clean_eng_text(x))\n",
        "val_data[\"Ger\"] = val_data[\"Ger\"].apply(lambda x: clean_ger_text(x))\n",
        "\n",
        "test_data[\"English\"] = test_data[\"English\"].apply(lambda x: clean_eng_text(x))\n",
        "test_data[\"Ger\"] = test_data[\"Ger\"].apply(lambda x: clean_ger_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1G2iROqScq0c"
      },
      "outputs": [],
      "source": [
        "import en_core_web_sm, de_core_news_sm\n",
        "\n",
        "# 단어 사전 형성\n",
        "class VocabGenerator:\n",
        "    def __init__(self, corpus, min_frequency=2, tokenizer_lang=\"english\"):\n",
        "\n",
        "        self.stoi = {'<UNK>':1,'<PAD>':0,'<SOS>':2,'<EOS>':3}          # 문자열을 index 변환\n",
        "        self.itos = {1:'<UNK>',0:'<PAD>',2:'<SOS>',3:'<EOS>'}          # index를 문자열로 변환\n",
        "        self.min_freq = min_frequency\n",
        "\n",
        "        # 영어, 독일어 문장을 각각 개별 토큰으로 변환\n",
        "        self.lang = tokenizer_lang\n",
        "        if tokenizer_lang == \"english\":\n",
        "            self.tokenizer = en_core_web_sm.load()\n",
        "        else:\n",
        "            self.tokenizer = de_core_news_sm.load()\n",
        "\n",
        "        self.build_vocabulary(corpus)\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        return len(self.itos)\n",
        "\n",
        "    def counter(self,text):\n",
        "\n",
        "        out = {}\n",
        "        text = text.split()\n",
        "        for word in text:\n",
        "            if word in out:\n",
        "                out[word] += 1\n",
        "            else:\n",
        "                out[word] = 1\n",
        "        return out\n",
        "\n",
        "    def build_vocabulary(self,corpus):\n",
        "\n",
        "        idx = 4\n",
        "        frequency_dict = self.counter(corpus)\n",
        "        for word,freq in frequency_dict.items():\n",
        "          if freq >= self.min_freq:\n",
        "              self.stoi[word] = idx\n",
        "              self.itos[idx] = word\n",
        "              idx += 1\n",
        "\n",
        "    def generate_numeric_tokens(self,text):\n",
        "\n",
        "        tokenized_out = [token.text for token in self.tokenizer(text)]\n",
        "        out = [self.stoi[token] if token in self.stoi.keys() else self.stoi[\"<UNK>\"] for token in tokenized_out]\n",
        "        return out\n",
        "\n",
        "    def add_eos_sos(self,numeric_list):\n",
        "\n",
        "        numeric_list = [self.stoi['<SOS>']] + numeric_list + [self.stoi['<EOS>']]      # 토큰에서 생성된 정수 목록\n",
        "        return numeric_list\n",
        "\n",
        "    def pad_sequence(self,numeric_list,max_seq_length):\n",
        "      # 토큰의 정수 목록을 패딩하고 잘라내는 함\n",
        "\n",
        "        if len(numeric_list) < max_seq_length:\n",
        "            no_zeros = max_seq_length - len(numeric_list)\n",
        "            numeric_list = numeric_list + [self.stoi['<PAD>'] for i in range(no_zeros)]\n",
        "        else:\n",
        "            numeric_list = numeric_list[:max_seq_length]\n",
        "\n",
        "        return numeric_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9jQwsZscu-s"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "max_seq_len=50\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, data, src_vocab, trg_vocab):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab\n",
        "        self.trg_vocab = trg_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def preprocess(self, text, lang):\n",
        "        tokenized_out = [token.text for token in lang(text)]\n",
        "        out = [self.src_vocab.stoi[token] if token in self.src_vocab.stoi.keys() else self.src_vocab.stoi[\"\"] for token in tokenized_out]\n",
        "        return out\n",
        "\n",
        "    def add_eos_sos(self, numeric_list):\n",
        "        numeric_list = [self.src_vocab.stoi['']] + numeric_list + [self.src_vocab.stoi['']]\n",
        "        return numeric_list\n",
        "\n",
        "    def pad_sequence(self, numeric_list, max_seq_length):\n",
        "        if len(numeric_list) < max_seq_length:\n",
        "            no_zeros = max_seq_length - len(numeric_list)\n",
        "            numeric_list = numeric_list + [self.src_vocab.stoi[''] for i in range(no_zeros)]\n",
        "        else:\n",
        "            numeric_list = numeric_list[:max_seq_length]\n",
        "        return numeric_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        src_text = self.data['src_column_name'].iloc[idx]\n",
        "        trg_text = self.data['trg_column_name'].iloc[idx]\n",
        "\n",
        "        src_tokens = self.preprocess(src_text, self.src_vocab)\n",
        "        trg_tokens = self.preprocess(trg_text, self.trg_vocab)\n",
        "        src_tokens = self.add_eos_sos(src_tokens)[:max_seq_len-2]\n",
        "        trg_tokens = self.add_eos_sos(trg_tokens)[:max_seq_len-2]\n",
        "\n",
        "        src_tokens = self.pad_sequence(src_tokens, max_seq_len)\n",
        "        trg_tokens = self.pad_sequence(trg_tokens, max_seq_len)\n",
        "\n",
        "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnd1lcTocx5u"
      },
      "outputs": [],
      "source": [
        "corpus_eng = ' '.join(list(train_data['English'].values))\n",
        "TRG = VocabGenerator(corpus=corpus_eng, min_frequency=2,tokenizer_lang=\"english\")\n",
        "\n",
        "corpus_ger = ' '.join(list(train_data['Ger'].values))\n",
        "SRC = VocabGenerator(corpus=corpus_ger, min_frequency=2,tokenizer_lang=\"germen\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYyXAlWebfDJ",
        "outputId": "46eddc36-1a21-4fd1-f854-751aac102bde"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len(SRC): 7971\n",
            "len(TRG): 5918\n"
          ]
        }
      ],
      "source": [
        "print(f\"len(SRC): {len(SRC)}\")\n",
        "print(f\"len(TRG): {len(TRG)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gUj7slpc12S",
        "outputId": "d625b178-0663-4931-ebbf-d8a24b5fb222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "2\n",
            "3\n",
            "264\n",
            "4070\n"
          ]
        }
      ],
      "source": [
        "print(TRG.stoi[\"<PAD>\"])  # 패딩(padding): 1\n",
        "print(TRG.stoi[\"<SOS>\"])        #  : 2\n",
        "print(TRG.stoi[\"<EOS>\"])       # : 3\n",
        "print(TRG.stoi[\"water\"])\n",
        "print(TRG.stoi[\"world\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7hoiD5fbfFx"
      },
      "outputs": [],
      "source": [
        "tokenizers = {\n",
        "    SRC: lambda x: [SRC.stoi[tok] for tok in SRC.generate_numeric_tokens(x)],\n",
        "    TRG: lambda x: [TRG.stoi[tok] for tok in TRG.generate_numeric_tokens(x)]\n",
        "}\n",
        "\n",
        "BOS, EOS, PAD = 2, 3, 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmiCH5W-c6qN"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "max_seq_len=50\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    'create a dataset for torch.utils.data.DataLoader() '\n",
        "    def __init__(self, data, src_vocab, trg_vocab):\n",
        "        self.data = data\n",
        "        self.src_vocab = src_vocab  # Change SRC to src_vocab\n",
        "        self.trg_vocab = trg_vocab  # Change TRG to trg_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_text, trg_text = self.data.iloc[idx]\n",
        "\n",
        "        # 토큰화하고 문장의 처음과 끝에 각각 BOS, EOS를 추\n",
        "        src_tokens = [BOS] + self.src_vocab.generate_numeric_tokens(src_text)[:max_seq_len-2] + [EOS]\n",
        "        trg_tokens = [BOS] + self.trg_vocab.generate_numeric_tokens(trg_text)[:max_seq_len-2] + [EOS]\n",
        "\n",
        "        return torch.tensor(src_tokens), torch.tensor(trg_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsopSLsCc8re"
      },
      "outputs": [],
      "source": [
        "# 배치의 모든 문장이 동일한 길이를 갖도록 문장에 패딩을 추가하는 함수\n",
        "def pad_sequence(batch):\n",
        "\n",
        "    src_seqs = [src for src, trg in batch]\n",
        "    trg_seqs = [trg for src, trg in batch]\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_seqs,\n",
        "                                batch_first=True, padding_value=PAD)\n",
        "    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_seqs,\n",
        "                                batch_first=True, padding_value=PAD)\n",
        "    return src_padded, trg_padded\n",
        "\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eeK6MxPlc_0Z"
      },
      "outputs": [],
      "source": [
        "train_data = TranslationDataset(train_data, SRC, TRG)\n",
        "valid_data = TranslationDataset(val_data, SRC, TRG)\n",
        "test_data = TranslationDataset(test_data, SRC, TRG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmkYQdmbdB_x"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=pad_sequence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ-kkPskdERN"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, h, d_embed, dropout=0.0):\n",
        "        super(MultiHeadedAttention, self).__init__()\n",
        "        assert d_embed % h == 0\n",
        "        self.d_k = d_embed//h      # dimension of each attention head\n",
        "        self.d_embed = d_embed     # embedding dimension\n",
        "        self.h = h                 # numbers of attention head\n",
        "        self.WQ = nn.Linear(d_embed, d_embed)\n",
        "        self.WK = nn.Linear(d_embed, d_embed)\n",
        "        self.WV = nn.Linear(d_embed, d_embed)\n",
        "        self.linear = nn.Linear(d_embed, d_embed)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x_query, x_key, x_value, mask=None):\n",
        "        nbatch = x_query.size(0)           # batch size\n",
        "\n",
        "        # 1) Linear projections to get the multi-head query, key and value tensors\n",
        "        # x_query, x_key, x_value dimension: nbatch * seq_len * d_embed\n",
        "\n",
        "        query = self.WQ(x_query).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
        "        key   = self.WK(x_key).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
        "        value = self.WV(x_value).view(nbatch, -1, self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "        # 2) Self-attention\n",
        "        # scores dimensions (sequence length): nbatch * h * seq_len * seq_len\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1))/math.sqrt(self.d_k)\n",
        "\n",
        "        # 3) Mask out padding tokens and future tokens\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask, float('-inf'))\n",
        "        # p_atten dimensions: nbatch * h * seq_len * seq_len\n",
        "        p_atten = torch.nn.functional.softmax(scores, dim=-1)\n",
        "        p_atten = self.dropout(p_atten)\n",
        "        # x dimensions: nbatch * h * seq_len * d_k\n",
        "        x = torch.matmul(p_atten, value)\n",
        "        # x dimensions: nbtach * seq_len * d_embed\n",
        "        x = x.transpose(1, 2).contiguous().view(nbatch, -1, self.d_embed)\n",
        "        return self.linear(x)              # final linear layer\n",
        "\n",
        "class ResidualConnection(nn.Module):\n",
        "  '''residual connection: x + dropout(sublayer(layernorm(x))) '''\n",
        "  def __init__(self, dim, dropout):\n",
        "      super().__init__()\n",
        "      self.drop = nn.Dropout(dropout)\n",
        "      self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "  def forward(self, x, sublayer):\n",
        "      return x + self.drop(sublayer(self.norm(x)))\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    '''Encoder = token embedding + positional embedding -> a stack of N EncoderBlock -> layer norm'''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_embed = config.d_embed\n",
        "        self.tok_embed = nn.Embedding(config.encoder_vocab_size, config.d_embed)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))\n",
        "        self.encoder_blocks = nn.ModuleList([EncoderBlock(config) for _ in range(config.N_encoder)])\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.norm = nn.LayerNorm(config.d_embed)\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "        x = self.tok_embed(input)\n",
        "        x_pos = self.pos_embed[:, :x.size(1), :]\n",
        "        x = self.dropout(x + x_pos)\n",
        "        for layer in self.encoder_blocks:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "    '''EncoderBlock: self-attention -> position-wise fully connected feed-forward layer'''\n",
        "    def __init__(self, config):\n",
        "        super(EncoderBlock, self).__init__()\n",
        "        self.atten = MultiHeadedAttention(config.h, config.d_embed, config.dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(config.d_embed, config.d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.d_ff, config.d_embed)\n",
        "        )\n",
        "        self.residual1 = ResidualConnection(config.d_embed, config.dropout)\n",
        "        self.residual2 = ResidualConnection(config.d_embed, config.dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.residual1(x, lambda x: self.atten(x, x, x, mask=mask))\n",
        "        # position-wise fully connected feed-forward layer\n",
        "        return self.residual2(x, self.feed_forward)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    '''Decoder = token embedding + positional embedding -> a stack of N DecoderBlock -> fully-connected layer'''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.d_embed = config.d_embed\n",
        "        self.tok_embed = nn.Embedding(config.decoder_vocab_size, config.d_embed)\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_seq_len, config.d_embed))\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "        self.decoder_blocks = nn.ModuleList([DecoderBlock(config) for _ in range(config.N_decoder)])\n",
        "        self.norm = nn.LayerNorm(config.d_embed)\n",
        "        self.linear = nn.Linear(config.d_embed, config.decoder_vocab_size)\n",
        "\n",
        "    # Subsequent Masking\n",
        "    def future_mask(self, seq_len):\n",
        "        '''mask out tokens at future positions'''\n",
        "        mask = (torch.triu(torch.ones(seq_len, seq_len, requires_grad=False), diagonal=1)!=0).to(DEVICE)\n",
        "        return mask.view(1, 1, seq_len, seq_len)\n",
        "\n",
        "    def forward(self, memory, src_mask, trg, trg_pad_mask):\n",
        "        seq_len = trg.size(1)\n",
        "        trg_mask = torch.logical_or(trg_pad_mask, self.future_mask(seq_len))\n",
        "        x = self.tok_embed(trg) + self.pos_embed[:, :trg.size(1), :]\n",
        "        x = self.dropout(x)\n",
        "        for layer in self.decoder_blocks:\n",
        "            x = layer(memory, src_mask, x, trg_mask)\n",
        "        x = self.norm(x)\n",
        "        logits = self.linear(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    ''' EncoderBlock: self-attention -> position-wise feed-forward (fully connected) layer'''\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.atten1 = MultiHeadedAttention(config.h, config.d_embed)\n",
        "        self.atten2 = MultiHeadedAttention(config.h, config.d_embed)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(config.d_embed, config.d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.dropout),\n",
        "            nn.Linear(config.d_ff, config.d_embed)\n",
        "        )\n",
        "        self.residuals = nn.ModuleList([ResidualConnection(config.d_embed, config.dropout)\n",
        "                                       for i in range(3)])\n",
        "\n",
        "    def forward(self, memory, src_mask, decoder_layer_input, trg_mask):\n",
        "        x = memory\n",
        "        y = decoder_layer_input\n",
        "        y = self.residuals[0](y, lambda y: self.atten1(y, y, y, mask=trg_mask))\n",
        "        # keys and values are from the encoder output\n",
        "        y = self.residuals[1](y, lambda y: self.atten2(y, x, x, mask=src_mask))\n",
        "        return self.residuals[2](y, self.feed_forward)\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, src_mask, trg, trg_pad_mask):\n",
        "        return self.decoder(self.encoder(src, src_mask), src_mask, trg, trg_pad_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NAESPQ4dIgM",
        "outputId": "d804b71e-2c0a-452d-caa0-9cf5db7dcee0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FBi8rvhc4bu"
      },
      "outputs": [],
      "source": [
        "class ModelConfig:\n",
        "    def __init__(self, encoder_vocab_size, decoder_vocab_size, d_embed, d_ff, h,\n",
        "                 N_encoder, N_decoder, max_seq_len, dropout):\n",
        "        self.encoder_vocab_size = encoder_vocab_size\n",
        "        self.decoder_vocab_size = decoder_vocab_size\n",
        "        self.d_embed = d_embed\n",
        "        self.d_ff = d_ff\n",
        "        self.h = h\n",
        "        self.N_encoder = N_encoder\n",
        "        self.N_decoder = N_decoder\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.dropout = dropout\n",
        "\n",
        "def make_model(config):\n",
        "    model = Transformer(Encoder(config), Decoder(config)).to(DEVICE)\n",
        "\n",
        "    for p in model.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU-8oFnrdKe0"
      },
      "outputs": [],
      "source": [
        "def make_batch_input(x, y):\n",
        "        src = x.to(DEVICE)\n",
        "        trg_in = y[:, :-1].to(DEVICE)\n",
        "        trg_out = y[:, 1:].contiguous().view(-1).to(DEVICE)\n",
        "        # padding tokens in the scr and trg sequences\n",
        "        src_pad_mask = (src == PAD).view(src.size(0), 1, 1, src.size(-1))\n",
        "        trg_pad_mask = (trg_in == PAD).view(trg_in.size(0), 1, 1, trg_in.size(-1))\n",
        "        return src, trg_in, trg_out, src_pad_mask, trg_pad_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xOwGn090dOcq"
      },
      "outputs": [],
      "source": [
        "en_vocab_size = 8200\n",
        "de_vocab_size = 10000\n",
        "vocab_sizes = {\"TRG\": en_vocab_size, \"SRC\": de_vocab_size}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lVF-XV8dQJD"
      },
      "outputs": [],
      "source": [
        "input_vocab_dim = SRC.__len__()\n",
        "target_vocab_dim = TRG.__len__()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RdlwCVgTdTjI",
        "outputId": "21cde01f-b0de-48a6-a0a9-5993c0e965fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_size: 22823198, train_set_size: 29056\n"
          ]
        }
      ],
      "source": [
        "config = ModelConfig(encoder_vocab_size = input_vocab_dim,\n",
        "                     decoder_vocab_size=target_vocab_dim,\n",
        "                     d_embed = 512,\n",
        "                     d_ff = 512,\n",
        "                     h = 8,\n",
        "                     N_encoder = 3,\n",
        "                     N_decoder = 3,\n",
        "                     max_seq_len = max_seq_len,\n",
        "                     dropout=0.1\n",
        "                     )\n",
        "\n",
        "train_size = len(train_loader) * batch_size\n",
        "model = make_model(config)\n",
        "model_size = sum([p.numel() for p in model.parameters()])\n",
        "print(f'model_size: {model_size}, train_set_size: {train_size}')\n",
        "warmup_steps = 3 * len(train_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prrRPMdvdVNG",
        "outputId": "7e559e9a-5ec9-42f3-b31a-26fe68a71252"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder): Encoder(\n",
              "    (tok_embed): Embedding(7971, 512)\n",
              "    (encoder_blocks): ModuleList(\n",
              "      (0-2): 3 x EncoderBlock(\n",
              "        (atten): MultiHeadedAttention(\n",
              "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (residual1): ResidualConnection(\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (residual2): ResidualConnection(\n",
              "          (drop): Dropout(p=0.1, inplace=False)\n",
              "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (tok_embed): Embedding(5918, 512)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (decoder_blocks): ModuleList(\n",
              "      (0-2): 3 x DecoderBlock(\n",
              "        (atten1): MultiHeadedAttention(\n",
              "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (atten2): MultiHeadedAttention(\n",
              "          (WQ): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WK): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (WV): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (linear): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.0, inplace=False)\n",
              "        )\n",
              "        (feed_forward): Sequential(\n",
              "          (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Dropout(p=0.1, inplace=False)\n",
              "          (3): Linear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (residuals): ModuleList(\n",
              "          (0-2): 3 x ResidualConnection(\n",
              "            (drop): Dropout(p=0.1, inplace=False)\n",
              "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "    (linear): Linear(in_features=512, out_features=5918, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "def initialize_weights(m):\n",
        "    if hasattr(m, 'weight') and m.weight.dim() > 1:\n",
        "        nn.init.xavier_uniform_(m.weight.data)\n",
        "\n",
        "model.apply(initialize_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zBeu4P8OdYN4"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.utils import lookfor\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train_epoch(model, dataloader):\n",
        "    model.train()\n",
        "    grad_norm_clip = 1.0\n",
        "    losses = []\n",
        "    num_batches = len(dataloader)\n",
        "    pbar = tqdm(enumerate(dataloader), total=num_batches)\n",
        "    for idx, (x, y) in pbar:\n",
        "        optimizer.zero_grad()\n",
        "        src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x, y)\n",
        "        pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n",
        "        pred = pred.view(-1, pred.size(-1))\n",
        "        loss = loss_fn(pred, trg_out).to(DEVICE)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm_clip)\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        losses.append(loss.item())\n",
        "        # report progress\n",
        "        if idx > 0 and idx % 50 == 0:\n",
        "            pbar.set_description(f'train loss={loss.item():.3f}, lr={scheduler.get_last_lr()[0]:.5f}')\n",
        "    return np.mean(losses)\n",
        "\n",
        "\n",
        "def train(model, train_loader, valid_loader, epochs):\n",
        "    global early_stop_count\n",
        "    best_valid_loss = float('inf')\n",
        "    train_size = len(train_loader.dataset)\n",
        "    pad_token_idx = TRG.stoi['<PAD>']\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=pad_token_idx)\n",
        "    for ep in range(epochs):\n",
        "        train_loss = train_epoch(model, train_loader)\n",
        "        valid_loss = validate(model, valid_loader)\n",
        "\n",
        "        print(f'ep: {ep}: train_loss={train_loss:.5f}, valid_loss={valid_loss:.5f}')\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "        else:\n",
        "            if scheduler.last_epoch > 2 * warmup_steps:\n",
        "                early_stop_count -= 1\n",
        "                if early_stop_count <= 0:\n",
        "                    return train_loss, valid_loss\n",
        "    return train_loss, valid_loss\n",
        "\n",
        "\n",
        "def validate(model, dataloder):\n",
        "    'compute the validation loss'\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    with torch.no_grad():\n",
        "        for i, (x, y) in enumerate(dataloder):\n",
        "            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x,y)\n",
        "            pred = model(src, src_pad_mask, trg_in, trg_pad_mask).to(DEVICE)\n",
        "            pred = pred.view(-1, pred.size(-1))\n",
        "            losses.append(loss_fn(pred, trg_out).item())\n",
        "    return np.mean(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1nU8l8BjdbtI"
      },
      "outputs": [],
      "source": [
        "def translate(model, x):\n",
        "    'translate source sentences into the target language, without looking at the answer'\n",
        "    with torch.no_grad():\n",
        "        dB = x.size(0)\n",
        "        y = torch.tensor([[BOS]*dB]).view(dB, 1).to(DEVICE)\n",
        "        x_pad_mask = (x == PAD).view(x.size(0), 1, 1, x.size(-1)).to(DEVICE)\n",
        "        memory = model.encoder(x, x_pad_mask)\n",
        "        for i in range(max_seq_len):\n",
        "            y_pad_mask = (y == PAD).view(y.size(0), 1, 1, y.size(-1)).to(DEVICE)\n",
        "            logits = model.decoder(memory, x_pad_mask, y, y_pad_mask)\n",
        "            last_output = logits.argmax(-1)[:, -1]\n",
        "            last_output = last_output.view(dB, 1)\n",
        "            y = torch.cat((y, last_output), 1).to(DEVICE)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMESrf8cdqg5",
        "outputId": "e1d2ffe0-7865-4f95-f4f2-18bbe02c6984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/118.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.4/118.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2022.10.31)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.22.4)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.3)\n",
            "Installing collected packages: portalocker, colorama, sacrebleu\n",
            "Successfully installed colorama-0.4.6 portalocker-2.7.0 sacrebleu-2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install sacrebleu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtwaN9FDdMzU"
      },
      "outputs": [],
      "source": [
        "import sacrebleu\n",
        "\n",
        "def remove_pad(sent):\n",
        "    '''truncate the sentence if BOS is in it,\n",
        "     otherwise simply remove the padding tokens at the end'''\n",
        "    if sent.count('')>0:\n",
        "      sent = sent[0:sent.index('')+1]\n",
        "    while sent and sent[-1] == '':\n",
        "            sent = sent[:-1]\n",
        "    return sent\n",
        "\n",
        "def decode_sentence(sentence_ids, vocab):\n",
        "    'convert a list of numbers representing a tokenized sentence to a literal string'\n",
        "    sentence = [vocab.itos[id] for id in sentence_ids]\n",
        "    sentence = \" \".join(sentence).replace(\"▁\", \"\").strip().replace(\" .\", \".\")\n",
        "    return sentence\n",
        "\n",
        "def evaluate(model, DataLoader, num_batch=None):\n",
        "    'evaluate the model, and compute the BLEU score'\n",
        "    model.eval()\n",
        "    refs, cans, bleus = [], [], []\n",
        "    with torch.no_grad():\n",
        "        for idx, (x, y) in enumerate(DataLoader):\n",
        "            src, trg_in, trg_out, src_pad_mask, trg_pad_mask = make_batch_input(x, y)\n",
        "            translation = translate(model, src)\n",
        "            # Convert trg_out and translation to lists\n",
        "            refs = refs + [decode_sentence(trg_out[i].tolist(), TRG) for i in range(len(src))]\n",
        "            cans = cans + [decode_sentence(translation[i].tolist(), TRG) for i in range(len(src))]\n",
        "            if num_batch and idx >= num_batch:\n",
        "                break\n",
        "        print(min([len(x) for x in refs]))\n",
        "        bleus.append(sacrebleu.corpus_bleu(cans, [refs]).score)\n",
        "\n",
        "        # print some examples\n",
        "        for i in range(3):\n",
        "            print(f'src:  {decode_sentence(src[i].tolist(), SRC)}')\n",
        "            print(f'trg:  {decode_sentence(trg_out[i].tolist(), TRG)}')\n",
        "            print(f'pred: {decode_sentence(translation[i].tolist(), TRG)}')\n",
        "        return np.mean(bleus)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "9ui860bmj5VB",
        "outputId": "20f65ed1-014e-4804-c6c3-d8b225ced0a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 1/227 [00:22<1:25:27, 22.69s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-c09eccc7cd81>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<PAD>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mearly_stop_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f01d35c90405>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, valid_loader, epochs)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_token_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mvalid_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-f01d35c90405>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lr_fn = lambda step: config.d_embed**(-0.5) * min([(step + 1) ** (-0.5), (step + 1) * warmup_steps**(-1.5)])\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.5, betas=(0.9, 0.98), eps=1e-9)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_fn)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=TRG.stoi['<PAD>'])\n",
        "early_stop_count = 2\n",
        "train_loss, valid_loss = train(model, train_loader, valid_loader, epochs=1)\n",
        "test_loss = validate(model, test_loader)\n",
        "\n",
        "print(\"train set examples:\")\n",
        "train_bleu = evaluate(model, train_loader, 20)\n",
        "print(\"validation set examples:\")\n",
        "valid_bleu = evaluate(model, valid_loader)\n",
        "print(\"test set examples:\")\n",
        "test_bleu  = evaluate(model, test_loader)\n",
        "print(f'train_loss: {train_loss:.4f}, valid_loss: {valid_loss:.4f}, test_loss: {test_loss:.4f}')\n",
        "print(f'test_bleu: {test_bleu:.4f}, valid_bleu: {valid_bleu:.4f} train_bleu: {train_bleu:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_this_sentence(text: str):\n",
        "    'translate the source sentence in string format into the target language'\n",
        "    input = torch.tensor([[BOS] + tokenizers[SRC](text) + [EOS]]).to(DEVICE)\n",
        "    output = translate(model, input)\n",
        "    return decode_sentence(output[0], TRG)\n",
        "\n",
        "translate_this_sentence(\"Eine Gruppe von Menschen steht vor einem Iglu.\")"
      ],
      "metadata": {
        "id": "T4HG8a6oJlJy"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}